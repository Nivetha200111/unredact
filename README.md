# PDF Unredactor ğŸ”

A powerful Python tool that crawls websites to find PDFs, detects and attempts to recover redacted content, and uses AI to classify mentioned entities as **Politically Exposed Persons (PEPs)** or **Victims**.

## Features

- **ğŸŒ DFS Web Crawler** - Crawls websites using depth-first search to find topic-related PDFs
- **ğŸ“„ Redaction Detection** - Identifies multiple types of redactions:
  - Black box/rectangle redactions
  - Black highlight overlays
  - Image overlays covering text
  - White-out redactions
  - Pattern fills
- **ğŸ”“ Unredaction Techniques** - Attempts to recover hidden text using:
  - PDF layer analysis
  - Metadata extraction
  - OCR with image enhancement
  - Font analysis
  - Image processing (contrast, thresholding, edge detection)
- **ğŸ¤– AI Classification** - Uses GPT-4 to classify entities as:
  - **PEP** (Politically Exposed Person) - politicians, officials, executives
  - **VICTIM** - victims of fraud, abuse, or wrongdoing
- **ğŸ“Š Organized Reports** - Generates comprehensive tables in Excel/CSV format

## Installation

### 1. Clone and setup

```bash
cd unredact
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Install system dependencies

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install tesseract-ocr poppler-utils
```

**macOS:**
```bash
brew install tesseract poppler
```

**Arch Linux:**
```bash
sudo pacman -S tesseract poppler
```

### 3. Configure API key

Copy the template and add your OpenAI API key:

```bash
cp env.template .env
# Edit .env and add your OPENAI_API_KEY
```

## Usage

### Crawl a website for PDFs

```bash
python main.py --url "https://example.com/documents" --topic "fraud investigation"
```

### Process local PDFs

```bash
python main.py --pdf-dir "./my_documents" --topic "financial scandal"
```

### Full options

```bash
python main.py \
  --url "https://site.com/archive" \
  --topic "corruption case" \
  --depth 5 \
  --excel \
  --csv
```

### Command-line options

| Option | Short | Description |
|--------|-------|-------------|
| `--url` | `-u` | Base URL to start crawling from |
| `--pdf-dir` | `-d` | Local directory containing PDFs |
| `--topic` | `-t` | Topic keywords for relevance filtering |
| `--depth` | | Maximum crawl depth (default: 3) |
| `--excel` | | Export to Excel (default: True) |
| `--csv` | | Export to CSV files |
| `--quiet` | `-q` | Don't print tables to console |
| `--no-excel` | | Skip Excel export |

## Output

The tool generates several output files in the `output/` directory:

### Excel Report (`unredact_report_TIMESTAMP.xlsx`)
Contains multiple sheets:
- **Summary** - Overview of all processed documents
- **Redactions** - Details of each detected redaction
- **Entities** - Classified entities (PEP/Victim)
- **Emails** - All email addresses found with classifications

### Console Tables
Formatted tables showing:
- Document processing summary
- Redaction detection and recovery results
- Entity classifications with confidence scores
- Email address inventory

## How It Works

### 1. Web Crawling (DFS)
```
Starting URL â†’ Find links â†’ Follow depth-first
                â†“
          Find PDF links
                â†“
          Check relevance to topic
                â†“
          Download relevant PDFs
```

### 2. Redaction Detection
```
PDF Page â†’ Extract annotations â†’ Check for redaction marks
    â†“
    â†’ Extract drawings â†’ Find filled rectangles
    â†“
    â†’ Convert to image â†’ Detect black regions (OpenCV)
    â†“
    â†’ Find image overlays â†’ Check for solid color covers
```

### 3. Unredaction Pipeline
```
Redaction Area â†’ Try PDF layer extraction
      â†“
      â†’ Try metadata extraction
      â†“
      â†’ Try font/character analysis
      â†“
      â†’ Try OCR with enhancements:
         â€¢ Contrast enhancement (CLAHE)
         â€¢ Multiple thresholds
         â€¢ Edge detection
         â€¢ Color inversion
         â€¢ Morphological operations
```

### 4. AI Classification
```
Extracted Text â†’ GPT-4 Analysis
      â†“
Entity Extraction â†’ Name, Email, Context
      â†“
Classification:
  â€¢ PEP: Politicians, officials, executives, judges
  â€¢ VICTIM: Plaintiffs, fraud targets, whistleblowers
  â€¢ OTHER: Neutral parties
```

## Example Output

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          PROCESSING COMPLETE                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 PDFs Found:            15
 PDFs Processed:        15
 Redactions Detected:   47
 Text Recovered:        12
 Entities Classified:   23
 PEPs Identified:       8
 Victims Identified:    5
```

## Project Structure

```
unredact/
â”œâ”€â”€ main.py              # Main orchestration script
â”œâ”€â”€ crawler.py           # DFS web crawler
â”œâ”€â”€ pdf_processor.py     # PDF parsing and redaction detection
â”œâ”€â”€ unredactor.py        # Redaction recovery techniques
â”œâ”€â”€ ai_classifier.py     # GPT-4 entity classification
â”œâ”€â”€ table_generator.py   # Report generation
â”œâ”€â”€ config.py            # Configuration settings
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ env.template         # Environment template
â”œâ”€â”€ downloads/           # Downloaded PDFs
â”œâ”€â”€ output/              # Generated reports
â””â”€â”€ temp/                # Temporary processing files
```

## Configuration

Edit `config.py` to customize:

```python
# Crawler settings
MAX_DEPTH = 5              # How deep to crawl
REQUEST_DELAY = 1.0        # Seconds between requests

# Redaction detection
BLACK_THRESHOLD = 30       # RGB threshold for "black"
MIN_REDACTION_AREA = 100   # Minimum pixel area

# OCR settings
OCR_DPI = 300              # Resolution for OCR
OCR_LANG = "eng"           # Tesseract language

# AI
OPENAI_MODEL = "gpt-4-turbo-preview"
```

## Limitations & Ethics

âš ï¸ **Important Considerations:**

1. **Legal**: Ensure you have permission to access and process the PDFs
2. **Privacy**: Handle recovered personal information responsibly
3. **Accuracy**: Unredaction is not always possible; results should be verified
4. **Rate Limits**: The tool respects website rate limits; don't abuse

Some redactions cannot be recovered:
- Properly applied redactions that permanently remove underlying data
- Scanned documents with no underlying text layer
- Heavily compressed or low-quality PDFs

## License

MIT License - See LICENSE file for details.
